{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Face/Emotion Detection using Integrated Webcam</b>\n",
    "\n",
    "This program will use OpenCV, a trained face detection model in Caffe, and a custom Keras model for emotion recognition. Both models are pretrained. The face detection does a great job while the custom Keras model has a mediocre performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Step 1:</b><br/>\n",
    "Import required libraries. This includes OpenCV, numpy and os. Keras layers are imported later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 2:</b><br/>\n",
    "Read the caffe model and wieghts, both located in a subfolder named \"model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "base_dir = os.getcwd()\n",
    "prototxt_path = os.path.join(base_dir + '/model/deploy.prototxt')\n",
    "caffemodel_path = os.path.join(base_dir + '/model/weights.caffemodel')\n",
    "\n",
    "# Read the model\n",
    "model = cv2.dnn.readNetFromCaffe(prototxt_path, caffemodel_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 3:</b><br/>\n",
    "Keras libraries and model are defined here. It includes 4 2D convolution layers; then, the model weights are loaded. <br/> <b> Note that </b>Since face detection model exist, classification model is called \"model1\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Flatten\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "\n",
    "# Create the model\n",
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
    "model1.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.25))\n",
    "\n",
    "model1.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "model1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model1.add(Dropout(0.25))\n",
    "\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(1024, activation='relu'))\n",
    "model1.add(Dropout(0.5))\n",
    "model1.add(Dense(7, activation='softmax'))\n",
    "\n",
    "#Load the model weights\n",
    "model1.load_weights(base_dir + '/model/model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 4:</b><br/>\n",
    "The keras model is defined to categorize facial emotions into seven categories. These categories are defined in a dictionary which assigns them a label (alphabetical order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: \"Angry\", 1: \"Disgusted\", 2: \"Fearful\", 3: \"Happy\", 4: \"Neutral\", 5: \"Sad\", 6: \"Surprised\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Step 5:</b><br/>\n",
    "In the final step, OpenCV is used to read a video from the webcam, using VideoCaptured. <br/>\n",
    "<span style=\"color:#FF0000\">The first step</span> in processing the video stream is mean subtraction and scaling. This has been done using <b><i>blobFromImage</b></i> function in OpenCV. The argument it takes are shown in the code. Scale factor of 0 is used and swapRB is false. Note that OpenCV reads BGR colorful images and swapRB converts them to RGB.<br/>\n",
    "<span style=\"color:#0000ff\">The second step</span> is to use the processed image as an input image for trained face detection model. Then loop on the function (since performed on a live feed) and when probability of a face being in an image is more than 0.5 show the bounding box.\n",
    "\n",
    "<span style=\"color:#006a4e\">In the third and last step</span> use the detected face as an input for emotion classifier, convert it to 48 by 48 (size of the input for model1), predict the emotion and put it as a text on the detected frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Load the cascade\n",
    "#face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# To capture video from webcam. \n",
    "cap = cv2.VideoCapture(0)\n",
    "# To use a video file as input \n",
    "# cap = cv2.VideoCapture('filename.mp4')\n",
    "\n",
    "cv2.startWindowThread()\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Read the frame\n",
    "    _, img = cap.read()\n",
    "\n",
    "    #Preprocess the image, subtract mean and convert BGR to RGB\n",
    "    (h, w) = img.shape[:2]\n",
    "    blob = cv2.dnn.blobFromImage(image = cv2.resize(img, (300, 300)), \\\n",
    "                                 scalefactor = 1.0, size = (300, 300), \\\n",
    "                                 mean = (104.0, 177.0, 123.0), swapRB=True)\n",
    "    #Use the blob as an input to the face detection model\n",
    "    model.setInput(blob)\n",
    "    detections = model.forward()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    # Create frame around face\n",
    "\n",
    "    for i in range(0, detections.shape[2]):\n",
    "        box = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "        (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "\n",
    "        confidence = detections[0, 0, i, 2]\n",
    "\n",
    "      # If confidence > 0.5, show box around face\n",
    "        if (confidence > 0.5):\n",
    "            cv2.rectangle(img, (startX, startY), (endX, endY), (255, 255, 255), 2)\n",
    "\n",
    "            frame = img[startY:endY, startX:endX]\n",
    "    \n",
    "            \n",
    "            #Colorful image is converted to grayscale just for the classification step (emotion detection)\n",
    "            frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            #The input size of the classifier is 48 by 48 as it converted to 48 by 48\n",
    "            cropped_img = np.expand_dims(np.expand_dims(cv2.resize(frame_gray, (48, 48)), -1), 0)\n",
    "            prediction = model1.predict(cropped_img)\n",
    "            \n",
    "            #Select the category with the maximum probability that came from the softmax layer(prediction)\n",
    "            maxindex = int(np.argmax(prediction))\n",
    "            \n",
    "            #Putting text on the frame to show result of the classification\n",
    "            cv2.putText(img, emotion_dict[maxindex], (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            #Show the live feed on screen\n",
    "            cv2.imshow('Live', img)\n",
    "            \n",
    "    if cv2.waitKey(1) & 0xff == ord('q'):\n",
    "        break\n",
    "        \n",
    "        \n",
    "cv2.DestroyWindow('Live')\n",
    "\n",
    "# When everything done, release the capture\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
